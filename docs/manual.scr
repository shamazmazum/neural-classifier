@begin[ref=index](section)
   @title(Overview)
   @c(neural-classifier) is a library for working with neural networks and
   solving classification problem (and maybe other problems). For a basic use
   read README file on the
   @link[uri="http://github.com/shamazmazum/neural-classifier"](GitHub page).
   Here the API is described in more details.
@end(section)

@begin(section)
   @title(General information)
   Typical neural network lifecycle is the following:
   @begin(enum)
      @item(Adjust hyperparameters if you want. See
            @ref[id=hyper](Hyperparameters) section for details. Default values
            are usually OK.)
      @item(Create a @c(snakes) generator which will return data for the
            training step in the form @c((data-object . label)) (i.e. cons pair
            which contains a sample for training and a label which describes
            that sample).)
      @item(Create four functions: The first and the second translate a sample
            returned by the generator created in the previous step to
            @c(magicl:matrix/single-float) matrix with dimensions @c(Nx1). One
            of them will be used by @c(calculate) function and another one will
            be used during the training. The function used for training may make
            additional random transformations of the input data to extend
            training set. The third translates a label to
            @c(magicl:matrix/single-float) matrix with dimensions @c(Mx1) and
            the fourth translates a matrix with dimensions @c(Mx1)to a label.
            Obviously the third function is used for training and the
            fourth is used for classifing objects with trained net.)
      @item(Create a neural network with
            @c(neural-classifier:make-neural-network). The first parameter,
            @c(layout), must be a list @c((N ... M)), where @c(N) and @c(M) are
            taken from the previous step. Also is accepts created transformation
            functions.)
      @item(Train a neural network calling
            @c(neural-classifier:train-epoch). An epoch is finished when
            supplied generator doesn't have any more data (i.e. returns
            @c(snakes:generator-stop)).)
      @item(Repeat steps 2 and 5 to train for more epochs. You can use
            @c(neural-classifier:rate) function to control the accuracy of your
            network, so you know where to stop training.)
      @item(After the net is trained, call @c(neural-classifier:calculate) to
            pass your data through the net.)
   @end(enum)

Abstact example: find a human face on photo.

@begin[lang=lisp](code)
(defun load-image (pathname)
  ;; Assume that our images are PNGs prescaled to 50x50 pixels
  (let ((image (opticl:coerce-image
                (opticl:read-png-file pathname)
                'opticl:gray-image))
        (matrix (magicl:empty '(#.(* 50 50) 1)
                              :type 'single-float)))
    ;; Convert simple array to magicl:matrix/single-float column
    (loop for i below (* 50 50) do
      (setf (magicl:tref matrix i 0)
            (/ (row-major-aref image i) 255.0)))
    matrix))

(defun classify (output)
  (declare (type magicl:matrix/single-float output))
  ;; When the first output value is less than the second one, classify
  ;; the picture as face
  (if (< (magicl:tref output 0 0)
         (magicl:tref output 1 0))
      :face :no-face))

(defun label-to-matrix (label)
  (declare (type (member :face :no-face) label))
  ;; Convert labels to matrices
  (ecase label
    (:face    (magicl:from-list '(0f0 1f0) '(2 1)))
    (:no-face (magicl:from-list '(1f0 0f0) '(2 1)))))

(defun make-network ()
  (neural-classifier:make-neural-network
   ;; One input layer with 50x50 neurons
   ;; One hidden layer with 100 neurons
   ;; One output layer with 2 neurons.
   '(#.(* 50 50) 100 2)
   ;; This is used for image loading in trained net
   :input-trans #'load-image
   ;; This is used for image loading while training
   :input-trans% (alexandria:compose #'add-noise #'random-rotate #'load-image)
   ;; This is used to produce a label
   :output-trans #'classify
   ;; Produce a matrix from a label
   :label-trans #'label-to-matrix
   ;; Activation fnctions: relu in the hidden layer, softmax in the
   ;; output
   :activation-funcs '(:relu :softmax)))

(defun train-epochs (network n)
  "Train a network for n epochs"
  ;; Arrange our train data
  (let ((data (snakes:list->generator
               (mapcar
                (lambda (pathname)
                  (cons pathname
                        (if (face-p pathname)
                            :face :no-face)))
                *list-of-pictures*))))
    (loop repeat n
          ;; Train one epoch
          do (neural-classifier:train-epoch network data)
          ;; Collect accuracy of recognition.
          ;; You must use another set for validation data in real
          ;; use.
          collect (neural-classifier:rate network data))))

(defun classify-image (network pathname)
  ;; When network is trained, just pass your image to CALCULATE
  ;; to classify it.
  (neural-classifier:calculate network pathname))
@end(code)

If you want another example, look at @c(mnist/mnist.lisp) to see how digit
recognition works.
@end(section)

@begin[ref=hyper](section)
   @title(Hyperparameters)
   These hyperparameters are used in the library (@c(neural-classifier) package
   is omitted for brevity).

   @begin(table)
     @begin(row)
       @cell(Name)
       @cell(Meaning)
       @cell(Where/when used)
     @end(row)
     @begin(row)
       @cell(@c(*learn-rate*))
       @cell(How fast the network learns)
       @cell(While training with all optimizers)
     @end(row)
     @begin(row)
       @cell(@c(*decay-rate*))
       @cell(Parameter for L^2 regularization)
       @cell(While training)
     @end(row)
     @begin(row)
       @cell(@c(*minibatch-size*))
       @cell(Calculate cost function gradient using how many samples?)
       @cell(While training)
     @end(row)
     @begin(row)
       @cell(@c(*momentum-coeff*))
       @cell(Parameter for optimizer algorithms)
       @cell(While training using momentum-based optimizers (or RMSprop with
             slightly different meaning))
     @end(row)
   @end(table)

   @cl:with-package[name="neural-classifier"](
      @cl:doc(variable *learn-rate*)
      @cl:doc(variable *decay-rate*)
      @cl:doc(variable *minibatch-size*)
      @cl:doc(variable *momentum-coeff*)
   )
@end(section)

@begin(section)
   @title(Optimizers)
   By default, @c(neural-classifier:train-epoch) uses stochastic gradient descent
   (SGD) algorithm to minimize the cost function. There are other optimizers
   which can be used in learn process. You can create an optimizer with
   @c(neural-classifier:make-optimizer) function and pass it to
   @c(neural-classifier:train-epoch) function. This is a list of optimizers. All
   symbols are in @c(neural-classifier) package.

   @begin(table)
     @begin(row)
       @cell(Symbol)
       @cell(Optimizer)
     @end(row)
     @begin(row)
       @cell(@c(sgd-optimizer))
       @cell(SGD optimizer)
     @end(row)
     @begin(row)
       @cell(@c(momentum-optimizer))
       @cell(SGD with momentum)
     @end(row)
     @begin(row)
       @cell(@c(nesterov-optimizer))
       @cell(Nesterov accelerated gradient (NAG) optimizer)
     @end(row)
     @begin(row)
       @cell(@c(adagrad-optimizer))
       @cell(Adagrad optimizer)
     @end(row)
     @begin(row)
       @cell(@c(rmsprop-optimizer))
       @cell(RMSprop optimizer)
     @end(row)
   @end(table)

   Here is a plot showing how accuracy of classification of test data from
   fashion MNIST set varies with the number of training epochs. Networks used in
   this example have one hidden layer with 50 neurons. All activation functions
   are sigmoids. Accuracy are averaged from 3 independent runs. Hyperparameters
   used are @c(*learn-rate*) = 0.001, @c(*decay-rate*) = 0, @c(*minibatch-size*)
   = 20, @c(*momentum-coeff*) = 0.9.
   @image[src=optimizers.png]()
@end(section)

@begin(section)
   @title(API documentation)
   @u(Neural network class and accessors)
   @cl:with-package[name="neural-classifier"](
      @cl:doc(class neural-network)
   )
   @u(Functions)
   @cl:with-package[name="neural-classifier"](
      @cl:doc(function make-neural-network)
      @cl:doc(function make-optimizer)
      @cl:doc(function calculate)
      @cl:doc(function train-epoch)
      @cl:doc(function rate)
      @cl:doc(function idx-abs-max)
   )
@end(section)
