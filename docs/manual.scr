@begin[ref=index](section)
   @title(Overview)
   @c(neural-classifier) is a library for working with neural networks and
   solving classification problem (and maybe other problems). For a basic use
   read README file on the
   @link[uri="http://github.com/shamazmazum/neural-classifier"](GitHub page).
   Here the API is described in more detail.
@end(section)

@begin(section)
   @title(General information)
   Typical neural network lifecycle is the following:
   @begin(enum)
      @item(Create a @c(snakes) generator which will return data for the
            training step in the form @c((data-object . label)) (i.e. cons pair
            which contains a sample for training and a label which describes
            that sample).)
      @item(Create four functions: The first two functions translate a sample
            returned by the generator created in the previous step to
            @c(magicl:matrix/single-float) matrix with dimensions @c(Nx1) which
            serves as an input into neural network. One of these functions is
            used for training and another for operation of a trained network.
            The function used for training may make additional random
            transformations of the input data to extend a training set. The
            third function translates a label to @c(magicl:matrix/single-float)
            matrix with dimensions @c(Mx1) and the fourth translates a matrix
            with dimensions @c(Mx1)to a label. Obviously the third function is
            used for training and the fourth is used for classifing objects with
            trained net.)
      @item(Create a neural network with
            @c(neural-classifier:make-neural-network). The first parameter,
            @c(layout), must be a list @c((N ... M)) which contains numbers of
            neurons in each layer of the network, where @c(N) and @c(M) are
            taken from the previous step. Also pass the functions created in the
            previous step as arguments to @c(make-neural-network).)
      @item(Train a neural network calling
            @c(neural-classifier:train-epoch). An epoch is finished when
            supplied generator doesn't have any more data (i.e. returns
            @c(snakes:generator-stop)). This function accepts an optimizer as an
            optional parameter. Visit @ref[id=optimizers](optimizers) section
            for more information about optimizers.)
      @item(Repeat the previous step to train for a higher number of epochs. You
            can use @c(neural-classifier:rate) function to control the accuracy
            of your network, so you know where to stop training.)
      @item(After the net is trained, call @c(neural-classifier:calculate) to
            pass your data through the net.)
   @end(enum)

Abstact example: find a human face on photo.

@begin[lang=lisp](code)
(defun load-image (pathname)
  ;; Assume that our images are PNGs prescaled to 50x50 pixels
  (let ((image (opticl:coerce-image
                (opticl:read-png-file pathname)
                'opticl:gray-image))
        (matrix (magicl:empty '(#.(* 50 50) 1)
                              :type 'single-float)))
    ;; Convert simple array to magicl:matrix/single-float column
    (loop for i below (* 50 50) do
      (setf (magicl:tref matrix i 0)
            (/ (row-major-aref image i) 255.0)))
    matrix))

(defun classify (output)
  (declare (type magicl:matrix/single-float output))
  ;; When the first output value is less than the second one, classify
  ;; the picture as face
  (if (< (magicl:tref output 0 0)
         (magicl:tref output 1 0))
      :face :no-face))

(defun label-to-matrix (label)
  (declare (type (member :face :no-face) label))
  ;; Convert labels to matrices
  (ecase label
    (:face    (magicl:from-list '(0f0 1f0) '(2 1)))
    (:no-face (magicl:from-list '(1f0 0f0) '(2 1)))))

(defun make-network ()
  (neural-classifier:make-neural-network
   ;; One input layer with 50x50 neurons
   ;; One hidden layer with 100 neurons
   ;; One output layer with 2 neurons.
   '(#.(* 50 50) 100 2)
   ;; This is used for image loading in trained net
   :input-trans #'load-image
   ;; This is used for image loading while training
   :input-trans% (alexandria:compose #'add-noise #'random-rotate #'load-image)
   ;; This is used to produce a label
   :output-trans #'classify
   ;; Produce a matrix from a label
   :label-trans #'label-to-matrix
   ;; Activation fnctions: relu in the hidden layer, softmax in the
   ;; output layer.
   :activation-funcs (list (make-instance 'neural-classifier:leaky-relu :coeff 0.0)
                           (make-instance 'neural-classifier:softmax))))

(defun train-epochs (network n)
  "Train a network for n epochs"
  ;; Arrange our train data
  (let ((data (snakes:list->generator
               (mapcar
                (lambda (pathname)
                  (cons pathname
                        (if (face-p pathname)
                            :face :no-face)))
                *list-of-pictures*))))
    (loop repeat n
          ;; Train one epoch
          do (neural-classifier:train-epoch network data)
          ;; Collect accuracy of recognition.
          ;; You must use another set for validation data in real
          ;; use.
          collect (neural-classifier:rate network data))))

(defun classify-image (network pathname)
  ;; When network is trained, just pass your image to CALCULATE
  ;; to classify it.
  (neural-classifier:calculate network pathname))
@end(code)

If you want another example, look at @c(mnist/mnist.lisp) to see how digit
recognition works.
@end(section)

@begin[ref=optimizers](section)
   @title(Optimizers)
   By default, @c(neural-classifier:train-epoch) uses stochastic gradient descent
   (SGD) algorithm to minimize the cost function. There are other optimizers
   which can be used during learning. You can create an optimizer by
   instantiating of of the optimizer classes (which are subclasses of
   @c(neural-classifier:optimizer)) and pass it to
   @c(neural-classifier:train-epoch) function. A complete list of optimizers is
   below. A symbol \(f\) present in the documentation denotes the cost
   function. A learning rate is specified using @c(:η) initarg. Initargs @c(:β1)
   and @c(:β2) are common for optimizers with momentum and variable learning
   rate respectively.

   @cl:with-package[name="neural-classifier"](
      @cl:doc(class optimizer)
      @cl:doc(class sgd-optimizer)
      @cl:doc(class momentum-optimizer)
      @cl:doc(class nesterov-optimizer)
      @cl:doc(class adagrad-optimizer)
      @cl:doc(class rmsprop-optimizer)
      @cl:doc(class adam-optimizer)
   )

   Here is a plot showing how accuracy of classification of test data from
   fashion MNIST set varies with the number of training epochs. Networks used in
   this example have one hidden layer with 50 neurons. All activation functions
   are sigmoids. Accuracy are averaged from 3 independent runs.
   @image[src=optimizers.png]()
@end(section)

@begin(section)
   @title(Activation functions)
   Activation functions are inserted after each layer of a neural network. These
   activation functions are supported:

   @cl:with-package[name="neural-classifier"](
      @cl:doc(class sigmoid)
      @cl:doc(class %tanh)
      @cl:doc(class softmax)
      @cl:doc(class leaky-relu)
      @cl:doc(class %identity)
   )

   Activation functions differ in how they can be associated with layers of a
   network. The division is as follows:

   @cl:with-package[name="neural-classifier"](
      @cl:doc(class activation)
      @cl:doc(class hidden-layer-activation)
      @cl:doc(class output-layer-activation)
   )
@end(section)

@begin(section)
   @title(API documentation)
   @u(Neural network class and accessors)
   @cl:with-package[name="neural-classifier"](
      @cl:doc(class neural-network)
   )
   @u(Functions)
   @cl:with-package[name="neural-classifier"](
      @cl:doc(function make-neural-network)
      @cl:doc(function calculate)
      @cl:doc(function train-epoch)
      @cl:doc(function rate)
      @cl:doc(function idx-abs-max)
   )
@end(section)
