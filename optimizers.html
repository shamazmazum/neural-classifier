<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>
  Optimizers &ndash; neural-classifier
</title>
    <link rel="stylesheet" href="static/style.css"/>
    
  <link rel="stylesheet" href="static/highlight.css"/>
  <script src="static/highlight.js"></script>
  <script src="static/load-mathjax.js" async></script>
  <style>
   /* Highlight the current top-level TOC item, and hide the TOC of all other items */

   .toc a[data-node="optimizers"] {
       /*color: #AD3108;*/
   }

   .toc ol {
       display: none;
   }

   .toc li a[data-node="optimizers"] {
       font-weight: bold;
   }

   .toc li a[data-node="optimizers"] + ol {
       display: block;
   }

   .toc li a[data-node="optimizers"] + ol li {
       margin-left: 10px;
   }
  </style>

  </head>
  <body>
    
  <h1 class="doc-title">neural-classifier</h1>
  <article id="article" data-section="optimizers">
    <aside>
      <ol class="toc"><li><a href="index.html" data-node="index">Overview</a></li><li><a href="general-information.html" data-node="general-information">General information</a></li><li><a href="optimizers.html" data-node="optimizers">Optimizers</a></li><li><a href="activation-functions.html" data-node="activation-functions">Activation functions</a></li><li><a href="api-documentation.html" data-node="api-documentation">API documentation</a></li></ol>
    </aside>
    <main class="codex-section">
      <header>
        <h2 class="section-title">Optimizers</h2>
      </header>
      <div class="content">
        <p>
   By default, <code>neural-classifier:train-epoch</code> uses stochastic gradient descent
   (SGD) algorithm to minimize the cost function. There are other optimizers
   which can be used during learning. You can create an optimizer by
   instantiating of of the optimizer classes (which are subclasses of
   <code>neural-classifier:optimizer</code>) and pass it to
   <code>neural-classifier:train-epoch</code> function. A complete list of optimizers is
   below. A symbol \(f\) present in the documentation denotes the cost
   function. A learning rate is specified using <code>:η</code> initarg. Initargs <code>:β1</code>
   and <code>:β2</code> are common for optimizers with momentum and variable learning
   rate respectively.</p><p>   
      <div class="codex-doc-node codex-record codex-class"><code class="codex-name">optimizer</code><div class="codex-class-struct-slot-option-node"><table class="codex-class-struct-slot-option-table"><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Superclasses</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">(t)</code></td></tr><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Metaclass</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">standard-class</code></td></tr><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Default Initargs</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">nil</code></td></tr></table></div><div class="codex-docstring">Generic optimizer class. Not to be instantiated</div><ul class="codex-slot-list"><li class="codex-slot codex-class-slot"><code class="codex-name">learning-rate</code><div class="codex-docstring">Parameter which controls learning
speed of the neural network. Must be a small positive value.</div><div class="codex-class-struct-slot-option-node"><table class="codex-class-struct-slot-option-table"><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Allocation</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell">instance</td></tr><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Type</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">single-float</code></td></tr><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Initarg</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">:η</code></td></tr><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Readers</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">(optimizer-learning-rate)</code></td></tr></table></div></li><li class="codex-slot codex-class-slot"><code class="codex-name">minibatch-size</code><div class="codex-docstring">Number of samples in a
minibatch. An integer in the range 10-100 is good for this
parameter.</div><div class="codex-class-struct-slot-option-node"><table class="codex-class-struct-slot-option-table"><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Allocation</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell">instance</td></tr><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Type</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">alexandria:positive-fixnum</code></td></tr><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Initarg</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">:minibatch-size</code></td></tr><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Initform</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">40</code></td></tr><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Readers</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">(optimizer-minibatch-size)</code></td></tr></table></div></li><li class="codex-slot codex-class-slot"><code class="codex-name">decay-rate</code><div class="codex-docstring">A parameter used for L²
regularization. 0.0 is no regularization. Good values are 1-10 divided
by the dataset size.</div><div class="codex-class-struct-slot-option-node"><table class="codex-class-struct-slot-option-table"><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Allocation</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell">instance</td></tr><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Type</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">single-float</code></td></tr><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Initarg</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">:decay-rate</code></td></tr><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Initform</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">0.0</code></td></tr><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Readers</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">(optimizer-decay-rate)</code></td></tr></table></div></li></ul></div>
      <div class="codex-doc-node codex-record codex-class"><code class="codex-name">sgd-optimizer</code><div class="codex-class-struct-slot-option-node"><table class="codex-class-struct-slot-option-table"><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Superclasses</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">(optimizer t)</code></td></tr><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Metaclass</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">standard-class</code></td></tr><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Default Initargs</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">(:η 0.01)</code></td></tr></table></div><div class="codex-docstring">A basic stochastic gradient optimizer. A parameter
\(w\) of a neural network is updated as \(w_{n+1} = w_n - \eta
\nabla f(w_n)\).</div><ul class="codex-slot-list"></ul></div>
      <div class="codex-doc-node codex-record codex-class"><code class="codex-name">momentum-optimizer</code><div class="codex-class-struct-slot-option-node"><table class="codex-class-struct-slot-option-table"><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Superclasses</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">(momentum-memo-optimizer t)</code></td></tr><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Metaclass</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">standard-class</code></td></tr><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Default Initargs</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">(:η 0.01 :β1 0.9)</code></td></tr></table></div><div class="codex-docstring"><p>Stochastic gradient descent optimizer with
momentum. A parameter \(w\) of a neural network is updated with
respect to an accumulated momentum \(m\):</p><p>\(m_{n+1} = \beta_1 m_{n} + \eta \nabla f(w_n)\)</p><p>\(w_{n+1} = w_n - m_{n+1}\)</p></div><ul class="codex-slot-list"></ul></div>
      <div class="codex-doc-node codex-record codex-class"><code class="codex-name">nesterov-optimizer</code><div class="codex-class-struct-slot-option-node"><table class="codex-class-struct-slot-option-table"><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Superclasses</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">(momentum-memo-optimizer t)</code></td></tr><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Metaclass</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">standard-class</code></td></tr><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Default Initargs</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">(:η 0.01 :β1 0.9)</code></td></tr></table></div><div class="codex-docstring"><p>Nesterov optimizer: a stochastic gradient descent
with momentum and 'look-ahead'. A parameter \(w\) of a neural
network is updated with respect to an accumulated momentum \(m\):</p><p>\(m_{n+1} = \beta_1 m_{n} + \eta \nabla f(w_n - \beta_1 m_n)\)</p><p>\(w_{n+1} = w_n - m_{n+1}\)</p></div><ul class="codex-slot-list"></ul></div>
      <div class="codex-doc-node codex-record codex-class"><code class="codex-name">adagrad-optimizer</code><div class="codex-class-struct-slot-option-node"><table class="codex-class-struct-slot-option-table"><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Superclasses</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">(rate-memo-optimizer t)</code></td></tr><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Metaclass</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">standard-class</code></td></tr><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Default Initargs</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">(:η 0.01)</code></td></tr></table></div><div class="codex-docstring"><p>Adagrad optimizer: an optimizer with decaying
learning rate. A parameter \(w\) of a neural network is updated as
follows:</p><p>\(s_{n+1} = s_n + (\nabla f(w_n))^2\)</p><p>\(w_{n+1} = w_n - \frac{\eta}{\sqrt{s_{n+1} + \epsilon}} \nabla f(w_n)\)</p></div><ul class="codex-slot-list"></ul></div>
      <div class="codex-doc-node codex-record codex-class"><code class="codex-name">rmsprop-optimizer</code><div class="codex-class-struct-slot-option-node"><table class="codex-class-struct-slot-option-table"><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Superclasses</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">(rate-memo-optimizer t)</code></td></tr><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Metaclass</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">standard-class</code></td></tr><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Default Initargs</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">(:η 0.001 :β2 0.99)</code></td></tr></table></div><div class="codex-docstring"><p>RMSprop optimizer: an optimizer with adaptive
learning rate.  A parameter \(w\) of a neural network is updated as
follows:</p><p>\(s_{n+1} = \beta_2 s_n + (1 - \beta_2)(\nabla f(w_n))^2\)</p><p>\(w_{n+1} = w_n - \frac{\eta}{\sqrt{s_{n+1} + \epsilon}} \nabla f(w_n)\)</p></div><ul class="codex-slot-list"></ul></div>
      <div class="codex-doc-node codex-record codex-class"><code class="codex-name">adam-optimizer</code><div class="codex-class-struct-slot-option-node"><table class="codex-class-struct-slot-option-table"><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Superclasses</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">(momentum-memo-optimizer rate-memo-optimizer t)</code></td></tr><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Metaclass</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">standard-class</code></td></tr><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Default Initargs</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">(:η 0.001 :β1 0.9 :β2 0.999)</code></td></tr></table></div><div class="codex-docstring"><p>ADAM optimizer: an optimizer with adaptive learning
rate and momentum.  A parameter \(w\) of a neural network is updated
as follows:</p><p>\(m_{n+1} = \beta_1 m_n + (1 - \beta_1) \nabla f(w_n)\)</p><p>\(s_{n+1} = \beta_2 s_n + (1 - \beta_2)(\nabla f(w_n))^2\)</p><p>\(\hat{m} = m_{n+1} / (1 - \beta_1^n) \)</p><p>\(\hat{s} = s_{n+1} / (1 - \beta_2^n) \)</p><p>\(w_{n+1} = w_n - \frac{\eta}{\sqrt{\hat{s} + \epsilon}} \hat{m}\)</p></div><ul class="codex-slot-list"><li class="codex-slot codex-class-slot"><code class="codex-name">corrected-momentum-coeff</code><div class="codex-docstring">Corrected \(\beta_1\) parameter</div><div class="codex-class-struct-slot-option-node"><table class="codex-class-struct-slot-option-table"><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Allocation</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell">instance</td></tr><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Type</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">single-float</code></td></tr><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Initform</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">1.0</code></td></tr><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Accessors</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">(optimizer-corrected-momentum-coeff)</code></td></tr></table></div></li><li class="codex-slot codex-class-slot"><code class="codex-name">corrected-rate-coeff</code><div class="codex-docstring">Corrected \(\beta_2\) parameter</div><div class="codex-class-struct-slot-option-node"><table class="codex-class-struct-slot-option-table"><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Allocation</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell">instance</td></tr><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Type</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">single-float</code></td></tr><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Initform</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">1.0</code></td></tr><tr class="codex-class-struct-slot-option-row"><td class="codex-class-struct-slot-option-label-cell">Accessors</td><td class="codex-class-struct-slot-option-value-cell codex-class-struct-slot-option-symbol-list-cell"><code class="codex-class-struct-slot-symbol-list">(optimizer-corrected-rate-coeff)</code></td></tr></table></div></li></ul></div>
   </p><p>   Here is a plot showing how accuracy of classification of test data from
   fashion MNIST set varies with the number of training epochs. Networks used in
   this example have one hidden layer with 50 neurons. All activation functions
   are sigmoids. Accuracy are averaged from 3 independent runs.
   <img src="optimizers.png"/>
</p>
      </div>
    </main>
  </article>
  <footer>
    <div class="info">
      Created with <a href="https://github.com/CommonDoc/codex">Codex</a>.
    </div>
  </footer>
  <script>
   HighlightLisp.highlight_auto();
  </script>

  </body>
</html>
