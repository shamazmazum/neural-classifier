<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>
  Hyperparameters &ndash; neural-classifier
</title>
    <link rel="stylesheet" href="static/style.css"/>
    
  <link rel="stylesheet" href="static/highlight.css"/>
  <script src="static/highlight.js"></script>
  <style>
   /* Highlight the current top-level TOC item, and hide the TOC of all other items */

   .toc a[data-node="hyper"] {
       /*color: #AD3108;*/
   }

   .toc ol {
       display: none;
   }

   .toc li a[data-node="hyper"] {
       font-weight: bold;
   }

   .toc li a[data-node="hyper"] + ol {
       display: block;
   }

   .toc li a[data-node="hyper"] + ol li {
       margin-left: 10px;
   }
  </style>

  </head>
  <body>
    
  <h1 class="doc-title">neural-classifier</h1>
  <article id="article" data-section="hyper">
    <aside>
      <ol class="toc"><li><a href="index.html" data-node="index">Overview</a></li><li><a href="general-information.html" data-node="general-information">General information</a></li><li><a href="hyper.html" data-node="hyper">Hyperparameters</a></li><li><a href="optimizers.html" data-node="optimizers">Optimizers</a></li><li><a href="activation-functions.html" data-node="activation-functions">Activation functions</a></li><li><a href="api-documentation.html" data-node="api-documentation">API documentation</a></li></ol>
    </aside>
    <main class="codex-section">
      <header>
        <h2 class="section-title">Hyperparameters</h2>
      </header>
      <div class="content">
        <p>
   These hyperparameters are used in the library (<code>neural-classifier</code> package
   is omitted for brevity).</p><p>   </p><table><tr><td>Name</td>
       <td>Meaning</td>
       <td>Where/when used</td>
     </tr>
     <tr><td><code>*learn-rate*</code></td>
       <td>How fast the network learns</td>
       <td>While training with all optimizers</td>
     </tr>
     <tr><td><code>*decay-rate*</code></td>
       <td>Parameter for L^2 regularization</td>
       <td>While training</td>
     </tr>
     <tr><td><code>*minibatch-size*</code></td>
       <td>Calculate cost function gradient using how many samples?</td>
       <td>While training</td>
     </tr>
     <tr><td><code>*momentum-coeff*</code></td>
       <td>Parameter for optimizer algorithms</td>
       <td>While training using momentum-based optimizers (or RMSprop with
             slightly different meaning)</td>
     </tr>
   </table><p>   
      <div class="codex-doc-node codex-variable"><code class="codex-name">*learn-rate*</code><div class="codex-docstring">Learning speed for gradient descent algorithms. Bigger values
result in faster learning, but too big is bad. Default value is good
for SGD, SGD with momentum and NAG optimizers. For Adagrad and
RMSprop try 0.001f0.</div></div>
      <div class="codex-doc-node codex-variable"><code class="codex-name">*decay-rate*</code><div class="codex-docstring">Regularization parameter <code>λ/N</code>, where <code>N</code> is the number of
objects in the training set and <code>λ</code> must be about 1-10. If not sure,
start with zero (which is the default).</div></div>
      <div class="codex-doc-node codex-variable"><code class="codex-name">*minibatch-size*</code><div class="codex-docstring">Number of samples to be used for one update of network parameters.</div></div>
      <div class="codex-doc-node codex-variable"><code class="codex-name">*momentum-coeff*</code><div class="codex-docstring">Hyperparameter for SGD optimizers which use momentum. Zero means
just usual SGD. RMSprop also uses this parameter in accumulation of
squared partial derivatives of network parameters. Good values are
0.8-0.9.</div></div>
   
</p>
      </div>
    </main>
  </article>
  <footer>
    <div class="info">
      Created with <a href="https://github.com/CommonDoc/codex">Codex</a>.
    </div>
  </footer>
  <script>
   HighlightLisp.highlight_auto();
  </script>

  </body>
</html>
